
İlk olarak orijinal SAM paper'ında model fine-tune edilemediğini söylüyor fakat örneklerde ve bizim burada izlediğimiz yol mask decoder'ının fine tune ederek istediğimiz herhangi bir objenin segmentini elde etmek oldu. 
Fine tune işleminden önce [[1.2 Datasets]] ve [[1.3.DataLoader]] kısımlarında veri setinin ve veri yüklemcisinin detayları işlenmiştir. Fine tune için HuggingFace'in Transformers([[1.1  HF Transformers SAM'in Fine Tune'u üzerine]]) Framework'ünü kullanıyoruz. Bu Framework'ün kullanılmasının sebebi Fine-tune işleminin kolay ve efektif olarak yapılabilmesi ve Flower Framework'üne [[1.6 FedSAM]] kısmında belirtilen uyumlu olarak entegre edilebilmesi. Aynı zamanda;
> [!info]
> -  Orijinal SAM modeli yerine bu modelin kullanılması konusunda ise HF Transformers kütüphanesi modelin hazır ağırlıkları ile fine tune edilebilmesine olanak sağlıyor. Hali hazırda bir çok SAM ile ilgili makale orijinal modelin fine tune edilmesi ile ilgilense de, bu projede HF Transformers kütüphanesinin SAM modeli kullanılmışıtr. Bu modelin kullanılmasının ikinci bir nedeni ise SAM modelinin Tensorflow versiyonudur. Bu versiyon ise Tensorflow'un Federe öğrenme kütüphanesi ile birlikte entegrasyonu mümkün olmakta.

Fine tune kısmında izlenilen yol aşağıdaki gibidir.

- İlk olarak [[1.3.DataLoader]]'ımı yüklemek için SamProcessor'ü kullanıyoruz. Processor'ün kullanımının nedeni ise model performansını arttırma ve görüntüleri SAM modeline beslemeden önce SAM İşlemcisi, görüntüleri sabit bir giriş boyutuna yeniden boyutlandırma, piksel değerlerini normalleştirme ve isteğe bağlı olarak görüntüleri belirli bir biçime dönüştürme gibi çeşitli ön işleme adımları gerçekleştirir. Ayrıca segmentasyona rehberlik eden noktalar veya sınırlayıcı kutular gibi görsel istemler hazırlar.

```python
# Initialize the processor
from transformers import SamProcessor
processor = SamProcessor.from_pretrained("facebook/sam-vit-base")
```


- Sonrasında ise [[1.0 Segment Anything Modeli Üzerine]] ve [[1.1  HF Transformers SAM'in Fine Tune'u üzerine]] kısmında anlatılan modelin yapısı üzerine başlıklarında da belirtildiği gibi model 3 kısımdan oluşmaktadır. Vision Encoder, Prompt Encoder ve Mask Decoder. 
- Aşağıdaki kodda da görüleceği üzere, en başta Vision Encoder ve Prompt Encoder kısmının eğitmini donduruyoruz. Bunun nedenleri ise;
    1. İlk olarak, modelin hazır eğitilmiş ve aşırı zengin özelliklerini korumamızdan dolayıdır.
	2. Sadece Mask Decoder eğitildiği için modelin kodlayıcıların genel özellik çıkarma yeteneklerini bozma riski olmadan belirli segmentasyon görevine daha iyi uyum sağlamasına olanak tanır.
	3. Verimlilik ve Hız örnek gösterilebilir. Parametre sayısı azaldığı için eğitim önemli ölçüde hızlanır.
	4. Overfitting'i önlemek. Veri seti az olduğu zaman, tüm parametrelerin güncellenmesi modelin eğitim setinden genelleme yapmak yerine gürültüyü ve belirli ayrıntıları öğrendiği aşırı uyuma yol açabilir. Önceden eğitilmiş bileşenlerin dondurulması, genelleştirilmiş özellik çıkarma yeteneklerini koruyarak bu riski azaltmaya yardımcı olur.


```python

from transformers import SamModel

model=SamModel.from_pretrained("./MainDir/UntitledFolder/checkpoint_sam_torch",local_files_only=True)


for name, param in model.named_parameters():

    if name.startswith("vision_encoder") or name.startswith("prompt_encoder"):

        param.requires_grad_(False)

```

- Loss function ve Optimizer; Loss fonksiyonu olarak genel olarak çoğu segmentasyon modelinde(UNET vb.) çoğu zaman daha verimli olduğu için DiceCoefficient Loss'u kullanılır. Biz de aynı şekilde DiceCoefficientLoss'u kullanıyoruz. Ve optimizer içinse Adam optimizer'ını kullanıyoruz.

```python
optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)

seg_loss = DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')
```

- Sonrasında ise training için 2 adet Tesla V100-PCIE-32GB kullandığımdan dolayı, modeli paralel olarak eğittik. 

- Sonrasında ise training'geçiyoruz. Burada aldığımız değerler pixel_values, input_boxes ve ground_truth_mask oluyor. Bu veriler kayıp hesabı ve modelin forward pass kısmında kullanılacaktır.

- Model, almış olduğu pixel_values, input_boxes verileri ile forward pass yapar.

```python
outputs = model(pixel_values=pixel_values, input_boxes=input_boxes, multimask_output=False)

```

- Loss hesaplama kısmında ise tahmin edilen maskelerin gereksiz boyutları sıkıştırılarak atılır ve loss değeri için gerçek segmentasyon mask'i ile DiceCoefficient Loss'u kullanılarak eşleştirilir.

```python
predicted_masks = outputs.pred_masks.squeeze(1)
loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))
```

- Validasyon kısmında da gradyan hesaplaması sıfırlanarak modelin eğitimine izin veirlmeden modelin validasyonu yapılır test veri setinden. Sonrasında da örnek bir resim alınıp her epokta gösterilir.

![[Pasted image 20240212191334.png]]



---

Links

https://huggingface.co/ybelkada/segment-anything

https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SAM/Fine_tune_SAM_(segment_anything)_on_a_custom_dataset.ipynb

https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SAM/Run_inference_with_MedSAM_using_HuggingFace_Transformers.ipynb