
Segment Anything modeli(SAM) Meta tarafından 2023 Nisan ayında yayınlanmış state-of-the-art bir segmentasyon modelidir. Model, bir giriş görüntüsü verildiğinde ilgilenilen herhangi bir nesnenin segmentasyon maskelerini tahmin etmek için kullanılabilir. 11 milyon lisanslı ve gizliliğe saygılı görüntü üzerinde 1 milyardan fazla maske ile bugüne kadarki en büyük segmentasyon veri seti ile eğitilmiştir. Model bir resim input'unu çoklu segment haline getirebiliyor ve aynı zamanda multiple bounding box ve multiple point coordinate'ları ile herhangi bir görüntünün segmentasyonunu gerçekleştirebiliyor.  Modelin yayınlanmayan versiyonunda ise prompt'lar aracılığı ile nesnenin segmentasyonu gerçekleştirilebilmekte fakat yayınlanmadığı için bundan söz etmeyeceğiz. 

Bu model pre-trained olup 3 adet versiyonu bulunmaktadır. vit-b, vit-m ve vit-h ağırlıkları ile yayınlanan modelde bizim kullanacağım diğerlerine göre hafif olan vit-b modeli. Bu modeli seçmemizin nedeni diğerlerine göre hafif, hızlı ve aynı zamanda task'ımız için en kararlı olanı. Modelin sadece mask decoder kısmı fine-tunable olduğundan dolayı [[1.4  Fine Tune of SAM]] kısmında genel olarak yoğunlaşacağımız kısım orası olacak. 

Model makalede de belirtildiği üzere üç kısımdan oluşuyor. Image encoder, flexible prompt encoder ve fast mask decoder. 

- Image encoder kısmı Mask Autoencoder pre trained Vision Transformer'ın minimal entegrasyonunun yüksek girdilerle eğitilmesinden sonra uygulanmasıdır. 
- Prompt encoder kısmı ise iki çeşit prompttan oluşmakta. sparse(points, boxes ve text) ve dense(masks). 
- Mask Decoder, image embedding, prompt embedding ve bir outpu tokeni bir maskeye verimli bir şekilde eşler.  Transformer kod çözücü bloğunun bir modifikasyonunu ve ardından dinamik bir maske tahmin kafasını kullanır. 








